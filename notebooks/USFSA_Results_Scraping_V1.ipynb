{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0770dd",
   "metadata": {},
   "source": [
    "## USFSA Results Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff4254",
   "metadata": {},
   "source": [
    "### Part 1: Scraping the Webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc1843",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559e25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78d678",
   "metadata": {},
   "source": [
    "#### Create a get request for the main URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c07e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = \"https://ijs.usfigureskating.org/leaderboard/nonqual_results/2022/32007/index.html\"\n",
    "#main_url = \"https://ijs.usfigureskating.org/leaderboard/nonqual_results/2022/32017/index.html\"\n",
    "#main_url = \"https://ijs.usfigureskating.org/leaderboard/nonqual_results/2022/32005/index.html\"\n",
    "request  = requests.get(main_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27aef8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "soup   = bs4.BeautifulSoup(request.text)\n",
    "events = soup.find_all(\"td\", attrs = {\"rowspan\": 1})\n",
    "links  = soup.find_all(\"td\", attrs = {\"class\": \"cm rb\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0689546",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = main_url.replace(\"index.html\", \"\")\n",
    "\n",
    "# Helper function to extract links\n",
    "def extract_link(x):\n",
    "    return x.find(\"a\")[\"href\"]\n",
    "\n",
    "# Extract the url ends for each webpage\n",
    "ends = list(map(extract_link, links))\n",
    "\n",
    "# Request urls for each webpage\n",
    "webpages = [baseurl + i for i in ends]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f070b9a",
   "metadata": {},
   "source": [
    "#### Function for processing one results page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc138aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse each results page\n",
    "def parse_results(html, team = False):\n",
    "    \n",
    "    '''\n",
    "    Takes an html text object containing the results of\n",
    "    one group\n",
    "    \n",
    "    :params html: html text\n",
    "    :returns: A DataFrame containing the place and university\n",
    "    for each start\n",
    "    '''\n",
    "    \n",
    "    # Create a soup object and extract the rows of the results table\n",
    "    soup = bs4.BeautifulSoup(html.text)\n",
    "    res  = soup.find_all(\"td\", attrs = {\"colspan\":1})\n",
    "    rows = soup.find_all(\"tr\")\n",
    "    rows = [x for x in rows if (len(x.find_all(\"td\")) == 9 or len(x.find_all(\"td\")) == 7)]\n",
    "    \n",
    "    # Extract the University names from each page\n",
    "    out = []\n",
    "    for i, x in enumerate(res):\n",
    "        if team:\n",
    "            uni = x.text\n",
    "        else:\n",
    "            uni = x.text.split(\", \")[-1]\n",
    "        out.append([rows[i].find(\"td\").text, uni, rows[i].find_all(\"td\")[-1].text])\n",
    "        \n",
    "    return pd.DataFrame(out, columns = [\"Place\", \"College\", \"Tie\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481b1af",
   "metadata": {},
   "source": [
    "#### Loop through each page and extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e35a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFS = []\n",
    "for i, x in enumerate(webpages):\n",
    "    temp = requests.get(x)\n",
    "    try:\n",
    "        data = parse_results(temp)\n",
    "        data = data.loc[~data[\"Tie\"].str.contains(\"Withdraw\")]\n",
    "        DFS.append(data)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5d3e6",
   "metadata": {},
   "source": [
    "### Part 2: Calculating the points awarded for each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4d8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = {13: [12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 1],\n",
    "          12: [12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "          11: [12, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "          10: [12, 10, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "          9: [12, 10, 8, 6, 5, 4, 3, 2, 1],\n",
    "          8: [12, 10, 8, 6, 4, 3, 2, 1],\n",
    "          7: [12, 10, 8, 6, 4, 2, 1],\n",
    "          6: [12, 10, 8, 6, 4, 2],\n",
    "          5: [10, 8, 6, 4, 2],\n",
    "          4: [8, 6, 4, 2],\n",
    "          3: [6, 4, 2],\n",
    "          2: [6, 4],\n",
    "          1: [6]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f05d2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = []\n",
    "for i, x in enumerate(DFS):\n",
    "    num = len(x)\n",
    "    \n",
    "    # Assign a number of points to each column\n",
    "    x = x.assign(points = lookup[num])\n",
    "\n",
    "    # Handle ties\n",
    "    temp = x.groupby(\"Place\")[\"points\"].transform(lambda x: x.mean())\n",
    "    x = x.assign(points = temp)\n",
    "    \n",
    "    # Handle championship event edge case\n",
    "    if \"Championship\" in events[i].text or \"International\" in events[i].text:\n",
    "        x[\"points\"] = x[\"points\"] + 2\n",
    "    \n",
    "    OUT.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2154eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL = pd.concat(OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e241490e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "College\n",
       "Univ. of CA Los Angeles     218.0\n",
       "University of Denver        214.0\n",
       "Univ. of CA Berkeley        193.0\n",
       "Univ. of CA San Diego       176.0\n",
       "Arizona State University    131.0\n",
       "Stanford University         127.5\n",
       "Univ. of CO Boulder         103.0\n",
       "CO State University          79.5\n",
       "Univ. of CO COSpgs           65.0\n",
       "Utah State University        63.0\n",
       "Colorado College             59.0\n",
       "Univ. of CA Irvine           38.0\n",
       "Western WA Univ.             27.0\n",
       "Univ. of Northern CO         17.0\n",
       "University of Wyoming        16.0\n",
       "Name: points, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FULL.groupby(\"College\")[\"points\"].sum().sort_values(ascending = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
